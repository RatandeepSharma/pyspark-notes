
  Agenda
  ------
  
   Prerequite: Python Programming Language

   Spark - Basics & Architecture
   Spark Core API (Low-level API)	
	-> RDD Transformations & Actions
	-> Shared Variables
   Spark SQL
	-> DataFrame Operations
   Machine Learning & Spark MLlib
   Introduction to Spark Streaming
  
   Materials
   ---------
	-> PDF Presentations
	-> Code Modules
	-> Class Notes
	-> GitHub: https://github.com/ykanakaraju/pyspark
   
   Spark
   -----
     -> Spark is written in 'Scala' programming language. 

     -> Spark is a unified in-memory distributed computing framework for big data analytics.	

     -> Spark is a polyglot
	-> Spark apps can be written in Scala, Java, Python, R  

     -> Spark programs can run on multiple cluster managers 
	-> local, spark standalone scheduler, YARN, Mesos, Kubernetes


   Cluster
   -------
	-> Is a unified entity consisting of a group of nodes who cumulative resources can be used
	   to store and process/compute data.  
		
	-> Distributed Storage : Splits the data that is to be stored on the disk into small chunks
	   called blocks and these blocks are distributed across lot of nodes in the cluster. 

       -> Distributed Computing: Distributing the computations across many machine and running your tasks
	  by using the resources (RAM & CPU cores) of many machine parallelly.

       -> In-memory computing -> in-memory distributing computing refers to the ability of the framework to
	  store the intermediate results in the RAM and subsequent tasks can read these results from memory 
	  and continue the processing. 

   Spark Unified Framework
   -----------------------
     Spark provides a consist set of API for processing different analytical workloads using the
     same execution engine and common data abstractions. 

	-> Batch Analytics of Unstructured data 	: Spark Core API (RDD API) 
	-> Batch Analytics of Structured data 		: Spark SQL
	-> Streaming Analytics (real-time)		: Spark Streaming, Structured Streaming
	-> Predictive Analytics (machine learning)	: Spark MLlib
	-> Graph parallel computations			: Spark GraphX

     
   Getting started with Spark 
   --------------------------
  
      1. Using your vLab
	
	 -> Login into your Lab (Windows Server)
	 -> Click on 'CentOS 7' icon and enter username and password (given in README.txt)
	 -> Now youur inside your lab

	 -> Open a terminal and connect to pyspark shell
		-> type "pyspark"

	 -> Open a terminal and connect to "Jupyter Notebook"
		-> jupyter notebook    (if this is not working, try the other command)
		-> jupyter notebook --allow-root


      2. Setting up your own PySpark environment 
	   
	   -> Install "Anaconda Navigator"   (URL: https://www.anaconda.com/products/individual)
	   -> You have two tools -> Jupyter Notebook & Spyder
	   -> To setup PySpark on Jupyter Notebook & Spyder follow the steps in the document shared in github.
	 
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


      3. Signup to Databricks community edition
		
             URL: https://databricks.com/try-databricks

		-> Signup using the above url
		-> Login with your userid and password.
		-> Go through "Guide: Quickstart tutorial"

     
   Spark Architecture
    ------------------

	1. Cluster Manager
		-> Jobs are submitted to CM
		-> Spark programs can be submitted to Spark Standalone, YARN, Mesos, Kubernetes
		-> CM will allocate executors to the application. 

	2. Driver
		-> Master Process
		-> Contains the SparkContext
		-> Maintain all the metadata of the user program and of RDD
		-> Send tasks to the executors

		Deploy Modes:
		 client  -> default, the driver process runs on the client machine.
		 cluster -> driver runs in one of the nodes in the cluster

	3. Executors
		-> Executes the tasks sent by the driver
		-> All tasks do the same processing, but on different partitions of the data
		-> They report the status of the tasks to the driver.

	4. SparkContext
		-> Starting point of execution 
		-> Created inside a driver
		-> Represents an application
		-> Link between the driver and several tasks running in the executors


   RDD (Resilient Distributed Dataset)
   ----------------------------------- 

	-> Is the fundamental data abstraction of spark

	-> Is a collection in distributed in-memory partitions
		-> Each partition is a collection of objects (of some type)

	-> RDDs are immutable

	-> RDD has two components
		-> Meta Data : RDD Lineage DAG
		-> Data      : A set of partitions

	-> RDDs are lazily evaluated.

	
    How to create RDDs ?
    --------------------
	
     Three ways:

	1. Create an RDD from some external data file

		rdd1 = sc.textFile( <filePath>, [numPartitions] )
		
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
		-> default number of partitions is given by the value of sc.defaultMinPartitions
		   sc.defaultMinPartitions has a value of 2 if your app has atleast 2 cores allocated.

		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)


	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize( [3,3,2,5,7,8,9,0,0,9,8,7,6,5,4,3,2,1,2,3,4,5,6,7] )
		-> default number of partitions is given by the value of sc.defaultParallelism
		  sc.defaultParallelism has a value equal to number of cores allocated.
		
		rdd1 = sc.parallelize( [3,3,2,5,7,8,9,0,0,9,8,7,6,5,4,3,2,1,2,3,4,5,6,7], 3 )


	3. By applying transformations on existing RDDs.

		rdd3 = rdd2.map(lambda x: x.upper())

   
    What can you do on an RDD ?
    ---------------------------

	Two things

	1. Transformations
		-> Only rdds are 'created'
		-> That means, a logical plan on how to create the RDD partitions (called Lineage DAG)
		   is created and maintained by driver.
		-> Actual executions of the tasks is not triggered.

	2. Actions
		-> Triggers execution
		-> Converts the logical plan to physical plan and the driver sends the required tasks
		   to the executors to compute the partitions of the RDD.

 
    RDD Lineage DAG
    ---------------	
    Lineage DAG is created when a transformation command is executed. 
    Lineage DAG of an RDD is a logical plan that tracks all the dependencies that causes the creation of
    this RDD all the way from the very first RDD.


	rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG of rdd1 : (4) rdd1 -> sc.textFile

	rdd2 = rdd1.flatMap(lambda x: x.upper())
	Lineage DAG of rdd2 : (4) rdd2 -> rdd1.flatMap -> sc.textFile

	rdd3 = rdd2.map(lambda x: x.split(" "))
	Lineage DAG of rdd3 : (4) rdd3 -> rdd2.map -> rdd1.flatMap -> sc.textFile

	rdd4 = rdd3.filter(lambda x: len(x) > 3)
	Lineage DAG of rdd3 : (4) rdd4 -> rdd3.filter ->  rdd2.map -> rdd1.flatMap  -> sc.textFile

  
   RDD Persistence
   ---------------
	rdd1 = sc.textFile(...)
	rdd2 = rdd1.t2(...) 
	rdd3 = rdd1.t3(...) 
	rdd4 = rdd3.t4(...) 
	rdd5 = rdd3.t5(...) 
	rdd6 = rdd5.t6(...) 
	rdd6.persist( StorageLevel.DISK_ONLY ) --> instruction to spark to persist the rdd6 partitions.
	rdd7 = rdd6.t7(...) 

	rdd6.collect()
	lineage rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile	  
	sc.textFile -> t3 -> t5 -> t6 -> rdd6 ==> collected...

	rdd7.collect()
	lineage rdd7 => rdd7 -> rdd6.t7 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile	  
	t7 -> rdd7 ==> collected.

	rdd6.unpersist()

	Storage Levels
        --------------
	1. MEMORY_ONLY   	-> (default) Serialized format, only in RAM
	2. MEMORY_AND_DISK	-> Serialized format, RAM if available, else DISK
	3. DISK_ONLY		-> Serialized on DISK
	4. MEMORY_ONLY_2	-> Serialized, memory with 2x replication
	5. MEMORY_AND_DISK_2
	
        Commands
        ---------
		rdd1.persist()     // memory serialized 1x replica
		rdd1.persist(StorageLevel.MEMORY_AND_DISK)
		rdd1.cache()

		rdd1.unpersist()


    Executor Memory Structure
    --------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  RDD Transformations
  -------------------
   => Every transformation returns RDD

	 
   1. map 		P: U -> V
			Object to Object transformation
			input RDD: N objects, output RDD: N objects

   2. filter		P: U -> Boolean
			Filter the objects of the input RDD based on the filter expression.
			input RDD: N objects, output RDD: >= N objects

	rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

   3. glom		P: None
			Returns one list object per partition with all the objects of the partition
			input RDD: N objects, output RDD: = numPartitions


		rdd1			rdd2 = rdd1.glom()
	
		P0 : 3,2,1,4,3,5 -> glom -> P0: [3,2,1,4,3,5]
		P1 : 5,3,2,4,3,6 -> glom -> P1: [5,3,2,4,3,6]
		P2 : 5,3,2,6,0,9 -> glom -> P2: [5,3,2,6,0,9]

		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)	

		rdd1.collect() => [3,2,1,4,3,5,5,3,2,4,3,6,5,3,2,6,0,9]   
		rdd1.glom().collect() => [[3,2,1,4,3,5], [5,3,2,4,3,6], [5,3,2,6,0,9]]
		rdd1.glom().map(len).collect() => [6,6,6]

   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the iterables produced by the function.
			input RDD: N objects, output RDD: > N objects
   
   		rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions		P: Itarable[U] -> Iterable[V]
				Entire partition is taken as function input and function transforms the input
				into another output iterable object

				Partition to partition transformation

		rdd1.mapPartitions(lambda x: [sum(x)] ).collect()
		rdd1.mapPartitions(lambda x: map(lambda a: a*10, x)).collect()



   6. mapPartitionsWithIndex	P: Int, Itarable[U] -> Iterable[V]
				Same as mapPartitions, but we partition-index is an additonal function
				parameter.
		
		rdd1.mapPartitionsWithIndex(lambda i, d: [(i, sum(d))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, x: map(lambda a: (i, a*10), x)).collect()


   7. distinct			P: None,  Optional: numPartitions
				Returns an RDD with distinct elements
		
		rdd2 = rdd1.distinct()
  	

   8. sortBy			P: U -> V, Optional: ascending (True/False), numPartitions
				Sorts the elements of the RDD based on function output

		rddWords.sortBy(len).collect()
		rddWords.sortBy(lambda x: x[0], False).collect()      # desc sort
		rdd1.sortBy(lambda x: x%6, True, 5).glom().collect()  # output RDD has 5 partitions
  
   Types of RDDs
	-> Generic RDDs:  RDD[U]
	-> Pair RDDs:     RDD[(U,V)]

   9. mapValues			P: U -> V
				Applied to Pair RDDs only
				Transforms the value part only by applying the function. Does not change
				the key.

	 rdd3.mapValues(lambda x : [x, x])	=> here x represents the 'value' part of (k,v) pairs

   10. groupBy			P: U -> V. Optional: numPartitions
				Returns a Pair RDD where the:
				key: the unique value of the function output
				value: ResultIterable of the elements of the RDD that produced the key.


		outputrdd = sc.textFile("E://Spark//wordcount.txt", 3) \
              			.flatMap(lambda x: x.split(" ")) \
              			.groupBy(lambda x: x) \
              			.mapValues(len) \
              			.sortBy(lambda x: x[1], False, 1)

   11. randomSplit		P: list of ratios (ex: [0.6, 0.4]), Optional: seed
				Returns a list of RDDs

		rddlist = rdd1.randomSplit([0.5, 0.5])
		rddlist = rdd1.randomSplit([0.5, 0.5], 7567)   $ here 7567 is the seed.


   12. repartition		P: numPartitions
				Is used to increase or decrease the number of partitions
				Cause global shuffle.

		rdd11 = rdd10.repartition(6)     # rdd11 will have 6 partitions
		rdd12 = rdd11.repartition(3)     # rdd12 will have 3 partitions


   13. coalesce			P: numPartitions
				Is used to decrease the number of partitions.
				Cause partition merging

		rdd11 = rdd10.coalesce(3)


   14. partitionBy		P: numPartitions, Optional: partitioning-function
				Applied only on PairRDDs and partitioning happens based on the 'key'

		rdd4 = rdd3.partitionBy(3)    # hash is used as the default partitioning function.
		rdd4 = rdd3.partitionBy(3, lambda x: x + 5)

     
   15. union, intersection, subtract & cartesian

	  Let us say rdd1 has M partitions & rdd2 has N partitions

	  command				# of output partitions
	 --------------------------------------------------------------
	 rdd1.union(rdd2)			M + N, narrow
	 rdd1.intersection(rdd2)		M + N, wide
	 rdd1.subtract(rdd2)			M + N, wide
	 rdd1.cartesian(rdd2)			M * N, wide


   ..ByKey Transformations
   -----------------------
     	-> Are wide transformations
	-> Applied only to pair RDD


    16. sortByKey		P: None, Optional: ascending (True/False), numPartitions
				Sorts the RDD based on the key

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(False).collect()
		rddPairs.sortByKey(True, 4).collect()


   17. groupByKey		P: None, Optional: numPartitions
				Returns a pair RDD with unique-keys and grouped values.
				NOTE: Avoid groupByKey if possible. Not efficient.

	         outputrdd = sc.textFile("E://Spark//wordcount.txt", 3) \
              			.flatMap(lambda x: x.split(" ")) \
				.map(lambda x: (x, 1)) \
              			.groupByKey() \
              			.mapValues(sum) \
              			.sortBy(lambda x: x[1], False, 1)


   18. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				Reduces all the values of each unique key by iterativly applying the reduce 
				function on different values of each unique key.

	outputrdd = sc.textFile("E://Spark//wordcount.txt", 3) \
              			.flatMap(lambda x: x.split(" ")) \
				.map(lambda x: (x, 1)) \
              			.reduceByKey(lambda x, y: x + y) \
              			.sortBy(lambda x: x[1], False, 1)   

    19. aggregateByKey	
	
		Three parameters: 
	
		1. zero-value : initial value based on the final value you want to produce.
		2. Sequence function: 
			-> merges all the values of each unique key in each partition with the zero-value
			-> We get one aggreated value per unique-key in each partition. 			        
		3. Combine function: 
			-> reduces all the aggregates values of each unique key per partition produced 
			   by seq.fn into one final value.

student_rdd = sc.parallelize([
  ("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  ("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  ("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  ("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  ("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  ("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  ("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 

avg_rdd = student_rdd.map(lambda a: (a[0], a[2])) \
            .aggregateByKey((0,0), 
                            lambda z,v: (z[0]+v, z[1]+1), 
                            lambda a,b:(a[0]+b[0], a[1]+b[1])) \
            .mapValues(lambda x: x[0]/x[1])



     20. joins	=> join, leftOuterJoin, rightOuterJoin, fullOuterJoin

			RDD[(U, V)].join( RDD[(U, W)] ) => RDD[(U, (V, W))]

			join = names1.join(names2)   #inner Join
			leftOuterJoin = names1.leftOuterJoin(names2)
			rightOuterJoin = names1.rightOuterJoin(names2)
			fullOuterJoin = names1.fullOuterJoin(names2)

     21. cogroup	=> Is used to join RDDs with duplicate keys
			   -> groupByKey -> fullOuterJoin

	rdd1 = [('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]
		=> (key1, [10,7]) (key2, [12,6]) (key3, [6])
	
        rdd2 = [('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

	rdd1.cogroup(rdd2) =>
		(key1, ([10,7], [5, 17])) (key2, ([12,6], [4,7])) (key3, ([6], [])) (key4, ([], [17]))

	
    Recommendations
    ---------------
	-> The size of each partition should be approx. 128 MB 
	-> The number of partitions should be a multiple of number of cores.
	-> Each executor should have 5 cores
	-> If the number of partitions is less than but close to 2000, then bump it up to more than 2000. 


   RDD Actions
   ------------
	1. collect

	2. count

	3. saveAsTextFile

	4. reduce		P: (U, U) -> U
				Reduces the entire RDD into one final value of the same type by iterativly
				applying the reduce function parallelly on every partition first and then reduce
				the output of all the partitions.
		rdd1				

		P0:  9, 7, 5, 3, 1         -> reduce  ->  -7 => 31
		P1:  0, 9, 5, 1, 2	   -> reduce  -> -17
		P2:  3, 8, 6, 4, 3, 2, 1   -> reduce  -> -21

		rdd1.reduce( lambda x, y: x - y ) = 31 

		
	5. aggregate

		Three parameters: 
	
		1. zero-value : initial value based on the final value you want to produce
		2. Sequence function: merges all the values of each partition with the zero-value
		3. Combine function: reduces all the output per partition produced by seq.fn into one final value.

		rdd1.aggregate( (0,0), lambda z,v : (z[0]+v, z[1]+1), lambda a,b: (a[0]+b[0], a[1]+b[1]) )	


	6. take			=> take(n)  ex: take(10)

        7. takeOrdered		=> rdd2.takeOrdered(20)
				   rdd2.takeOrdered(20, lambda x: x%4)

        8. takeSample
				rdd2.takeSample(True, 20)	  // withReplacement sampling
				rdd2.takeSample(True, 20, 465)    // 465 is a seed
				rdd2.takeSample(False, 20)	  // withoutReplacement sampling
				rdd2.takeSample(False, 20, 798)	  // 798 is a seed

       9. countByValue		rddWords.countByValue()
				defaultdict(<class 'int'>, 
				{'hadoop': 25, 'spark': 40, 'scala': 28, 'flatmap': 12, 'map': 6, 'groupby': 6, 'flume': 6, 'oozie': 6, 'sqoop': 6, 'hive': 19, 'mapreduce': 6, 'hdfs': 6, 'rdd': 43, 'transformations': 10, 'actions': 10, 'asd': 5, 'sadas': 1, 'das': 6, 'd': 1})

       10. countByKey		rddPairs.countByKey()

       11. foreach    		-> Executes a function on all objects of the RDD
			 	   Does not return anything

       12. first


   Use-Case
   --------
   From cars.tsv dataset, find out the average weight of each make of cars of American origin
   Arrange the data in the DESC order of average weight
   Save the output as a single text file. 
   
	Dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	=> Try to solve it.


   Closures
   --------
	A closure represents all the variables and methods that must be visible to an executor
	to perform its computations on an RDD. 

	The 'closure' is serialized and a copy of it is sent to every executor


	 c = 0   # counter

	 def isPrime(a) :
		return 1 if a is prime
		return 0 if n is not prime

	 def f1 (n) :
		global c
		if ( isPrime(n) == 1 ) c = c + 1
		return n*2	 

	 rdd1 = sc.parallelize( range(1, 4001), 4 )

	 rdd2 = rdd1.map( f1 )

	 output = rdd2.collect()

	 print(c)     // 0 


	Limitation:
	----------
	-> Local variables that are part of function closures can not be used to implement global
	   counters.
		-> Beacuse these are local copies with in every tasks and changes to these variables
		   can not be propagated back to driver.

	-> You CAN NOT USE local variables to impliment global counters. Use 'accumulator' for this purpose. 
	
   Shared Variables		
   -----------------

	Accumulator
	-----------
	Is a shared variable that is maintained by driver and can be added to by all the tasks
	Accumulator is not part of closure, hense it is not a local copy.
	

	 c = sc.accumulator(0)

	 def isPrime(a) :
		return 1 if a is prime
		return 0 if n is not prime

	 def f1 (n) :
		global c
		if ( isPrime(n) == 1 ) c.add(1)
		return n*2	 

	 rdd1 = sc.parallelize( range(1, 4001), 4 )

	 rdd2 = rdd1.map( f1 )

	 output = rdd2.collect()

	 print(c)     // 0 


	Broadcast
        ---------
	=> A broadcast variable is a single copy sent (broadcasted) to every executor node and all the
	   tasks running in that executor can access this single copy. 

	=> By converting a large immutable collection into a broadcast variable you can save a lot of 
	   execution memory.


	dict = {1:'a', 2:'b', 3:'c', 4:'d', 5:'e', 6:'f', .......}   // 100MB
        bcDict = sc.broadcast( dict )

	def f1( n ) :
	    global bcDict 
	    return bcDict.value[n]

	rdd1 = sc.parallelize([1,2,3,4,5,6,7,8,...], 4)

	rdd2 = rdd1.map( f1 )
	
	rdd2.collect()
  
	Example
	-------
	lookup = sc.broadcast({1: 'a', 2:'e', 3:'i', 4:'o', 5:'u'}) 
	result = sc.parallelize([2, 1, 3, 4, 5]).map(lambda x: lookup.value[x]) 
	print( result.collect() )


   Spark-Submit
   ------------

    -> Is a single command that is used to submit any spark application (scala, java, python, R)
       to any cluster manager (local, spark standalone scheduler, yarn, mesos, k8s)


    	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--executor-memory 10G
		--executor-cores 5
		--num-executors 20
		E:\pyspark\wordcount.py   [command-line-args]

       spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py	
       spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt output 1

  ===================================			
     Spark SQL  (pyspark.sql)
  ===================================

    -> Spark's high-level API for structured data processing.

    -> Supported input data formats:
		Strutured File Formats: Parquet (default), ORC, JSON, CSV (delimited text)
		Hive
		JDBC -> RDBMS, NoSQL


    -> Spark SQL executes the code using Spark SQL engine which uses optimizers such as catalyst & tungston.
       Hence Spark SQL is very efficient then directly working using Spark Core.

    -> SparkSession
	-> Starting point of execution. 
	-> Represents a user-session inside an application
		-> An application (sparkContext) can have multiple sessions (sparksession)

    		spark = SparkSession \
        		.builder \
        		.appName("Dataframe Operations") \
        		.config("spark.master", "local[*]") \
        		.getOrCreate()  

    -> DataFrame (DF)
	
	-> Is a collection of distribute in-memory partitions that are immutable and lazily evaluated.

	-> DataFrame is collection of "Row" objects. (pyspark.sql.Row)
		-> Row is  a collection of columns  (pyspark.sql.Column)
		
        -> DataFrame has two components:
		-> Data   : Collection of Rows
		-> Schema : Represents the structure of the row.
			    Schema is a StructType objects.

			StructType(
			    List(
				StructField(age,LongType,true),
				StructField(gender,StringType,true),
				StructField(name,StringType,true),
				StructField(phone,StringType,true),
				StructField(userid,LongType,true)
			    )
			)


    Working with Spark SQL
    ----------------------

        1. Read/Load the data from some source (external/internal) into a dataframe.

		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)

	2. Apply the transformations on the dataframe using DF API methods or using SQL

		DF API approach
		----------------
			
			df2 = df1.select("userid", "name", "age", "gender", "phone") \
        			.where("age is not null") \
        			.orderBy("age", "name") \
        			.groupBy("age").count() \
        			.orderBy("age") \
        			.limit(4)

		Using SQL
		---------
			df1.createOrReplaceTempView("users")
			spark.catalog.listTables()

			qry = """select age, count(*) as count
        			from users
       				where age is not null
        			group by age
        			order by count
        			limit 4"""
        
			df3 = spark.sql(qry)   

			df3.show()  


        3. Write/save the contents of the dataframe to some external directory / database etc.

		df3.write.format("json").save(outputDir)
		df3.write.json(outputDir)

		df3.write.format("json").save(outputDir, mode="overwrite")
		df3.write.json(outputDir, mode="overwrite")
    
		
   Save Modes
   -----------
	default: errorIfExists	

	-> ignore
	-> append
	-> overwrite

	df3.write.format("json").save(outputDir, mode="overwrite")
	df3.write.mode("overwrite").format("json").save(outputDir)

	
   LocalTempViews & GlobalTempViews
   ---------------------------------
	df1.createOrReplaceTempView("users")

        // global temp views
	df1.createGlobalTempView("gusers")

	qry = """select age, count(*) as count 
        	 from global_temp.gusers 
         	where age is not null
         	group by age
         	order by count
         	limit 4"""
         
	df3 = spark.sql(qry)
	df3.show()


	Local Temp View :
		-> Created in SparkSession's local catalog.
			df1.createOrReplaceTempView("users")
		-> Accessible only from with in that sparksession.

	Global Temp View:
		-> Created at application scope
			df1.createGlobalTempView("gusers")
		-> Accessible from any sparksession in that application.

	

   DataFrame Transformations
   --------------------------
	-> A DF transformation returns another DF.

   1. select

		df2 = df1.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME", "count")

		df2 = df1.select( col("DEST_COUNTRY_NAME").alias("destination"), 
                  	column("ORIGIN_COUNTRY_NAME").alias("origin"), 
                 	expr("count"),
                  	expr("count + 10 as newCount"),
                  	expr("count > 200 as highFrequency"),
                  	expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")
                )

		df2 = df1.select( expr("DEST_COUNTRY_NAME as destination"), 
                  expr("ORIGIN_COUNTRY_NAME as origin"), 
                  expr("count"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequency"),
                  expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")
                )		

   2. where / filter

		df3 = df2.where(col("count") > 1000)
		df3 = df2.filter(col("count") > 1000)
		df3 = df2.where("domestic = false and highFrequency = false")

   3. orderBy / sort
		
		df3 = df2.orderBy("count", "origin")
		df3 = df2.orderBy(desc("count"), asc("origin"))
		
		df3 = df2.sort(desc("count"), asc("origin"))

   4. groupBy  -> returns a GroupedData object. Apply some aggregation method to return a DataFrame

		df3 = df2.groupBy("domestic", "highFrequency").count()
		df3 = df2.groupBy("domestic", "highFrequency").max("count")
		df3 = df2.groupBy("domestic", "highFrequency").sum("count")
		df3 = df2.groupBy("domestic", "highFrequency").avg("count")

		df3 = df2.groupBy("domestic", "highFrequency") \
        		.agg( count("count").alias("count"),
              		      sum("count").alias("sum"),
              		      avg("count").alias("avg")
            		   )
		
   5. limit
		df2 = df1.limit(100)

   6. selectExpr
		
		df2 = df1.selectExpr( "DEST_COUNTRY_NAME as destination", 
                  "ORIGIN_COUNTRY_NAME as origin", 
                  "count",
                  "count + 10 as newCount",
                  "count > 200 as highFrequency",
                  "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"
                )

		IS SAME AS: 

		df2 = df1.select( expr("DEST_COUNTRY_NAME as destination"), 
                  expr("ORIGIN_COUNTRY_NAME as origin"), 
                  expr("count"),
                  expr("count + 10 as newCount"),
                  expr("count > 200 as highFrequency"),
                  expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")
                )


    7. withColumn

		df3 = df1.withColumn("newCount", col("count") + 10) \
        		.withColumn("highFrequency", expr("count > 200")) \
        		.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME"))

	
    8.  withColumnRenamed

		df4 = df3.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin") 


   9. drop
	      df3 = df2.drop("newCount", "highFrequency")
	

   10. union, intersect, subtract

		df3 = df1.where("DEST_COUNTRY_NAME = 'India'")
		df4 = df1.where("count > 1000")	

		df5 = df3.union(df4)
		df6 = df5.intersect(df3)
		df7 = df5.subtract(df4)

   11. sample
		df3 = df1.sample(True, 0.5)
		df3 = df1.sample(True, 0.5, 4567)
		df3 = df1.sample(True, 1.5, 4567)

		df3 = df1.sample(False, 0.5, 4567)

   12. distinct

		df4 = df3.distinct()
		df1.select("ORIGIN_COUNTRY_NAME").distinct().count()

   13. randomSplit
	
		dfList = df1.randomSplit([0.4, 0.3, 0.3], 7897)
		dfList[0].count()
		dfList[1].count()
		dfList[2].count()
	
		df10, df11, df12 = df1.randomSplit([0.4, 0.3, 0.3], 7897)
		df10.count()
		df11.count()
		df12.count()

   14. repartition

	df2 = df1.repartition(4)
	df2.rdd.getNumPartitions()

	df3 = df2.repartition(2)
	df3.rdd.getNumPartitions()

	df4 = df2.repartition( col("DEST_COUNTRY_NAME") )
	df4.rdd.getNumPartitions()

	df5 = df2.repartition( 8, col("DEST_COUNTRY_NAME") )
	df5.rdd.getNumPartitions()

   15. coalesce

	df6 = df5.coalesce(4)

   16. join -> discussed as separate topic.   


   Working with Structured File Formats
   ------------------------------------
   JSON
	read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.json(inputPath)

	write
		df3.write.format("json").save(outputDir)
		df3.write.json(outputDir)

   Parquet
	read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.parquet(inputPath)

	write
		df3.write.format("parquet").save(outputDir)
		df3.write.parquet(outputDir)

   ORC
	read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.orc(inputPath)

	write
		df3.write.format("orc").save(outputDir)
		df3.write.orc(outputDir)

   CSV (delimited text file)

	read
		df1 = spark.read.format("csv").save(inputPath, header=True, inferSchema=True)
		df1 = spark.read.format("csv").option("header", True).option("inferSchema", True).load(inputPath)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df2.write.csv(outputDir, mode="overwrite")  			   # header-less csv
		df2.write.csv(outputDir, mode="overwrite", header=True)		   # with header
		df2.write.csv(outputDir, mode="overwrite", header=True, sep="|")   # pipe-separated file


   Create an RDD from DataFrame
   ----------------------------
	rdd1 = df1.rdd  # returns an RDD of Row objects

	
   Create DataFrame form programmatic data
   ---------------------------------------
	listUsers = [(1, "Raju", 45),
             (2, "Ramesh", 35),
             (3, "Raghu", 25),
             (4, "Ravi", 35),
             (5, "Ramya", 45),
             (6, "Radhika", 35),
             (7, "Ramana", 25)]

	df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age")


   Create DataFrame from an RDD
   ----------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)
	rdd1.collect()

	df2 = spark.createDataFrame(rdd1).toDF("id", "name", "age")


   Create dataframe with custom schema (programmatic schema)
   ---------------------------------------------------------
	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
          ])

	df2 = spark.createDataFrame(rdd1, schema=mySchema)

	-------------------------------------------------

	inputPath = "E:\\PySpark\\data\\flight-data\\json\\2015-summary.json"

	mySchema = StructType([
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
          ])

	df1 = spark.read.json(inputPath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputPath)


   DataFrame Joins
   ---------------
    Supported Joins: inner, left_outer, right_outer, full_outer, left_semi, left_anti

     left-semi join :
     ----------------
	-> Similar to inner-join, but the data comes ONLY from the left table. 
	-> Is equivalent to the following sub-query:		
		select * from emp where deptid IN (select id from dept)

     left-anti join :
     ----------------
	-> Is equivalent to the following sub-query:		
		select * from emp where deptid NOT IN (select id from dept)
	
	data
	----
employee = spark.createDataFrame([
    (1, "Raju", 25, 101),
    (2, "Ramesh", 26, 101),
    (3, "Amrita", 30, 102),
    (4, "Madhu", 32, 102),
    (5, "Aditya", 28, 102),
    (6, "Pranav", 28, 100)])\
  .toDF("id", "name", "age", "deptid")
  
  
department = spark.createDataFrame([
    (101, "IT", 1),
    (102, "ITES", 1),
    (103, "Opearation", 1),
    (104, "HRD", 2)])\
  .toDF("id", "deptname", "locationid")


employee.show()
department.show()

	SQL way
        -------
	spark.catalog.listTables()
 
	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	qry = """select *
         	from emp left anti join dept
         	on emp.deptid = dept.id"""         
         
	joinedDf = spark.sql(qry)

	joinedDf.show()


	DataFrame API
        -------------
	# Supported Joins: inner (default), left_outer, right_outer, full_outer, left_semi, left_anti

	joinCol = employee["deptid"] == department["id"]
	joinedDf = employee.join(department, joinCol, "left_outer")
 

	Enforcing a broadcast join:

	  -> joinedDf = employee.join(broadcast(department), joinCol, "left_outer")

	     spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "20M")  => to set the value.
		-> default is "10 MB"


   Working with JDBC (MySQL)   
   -------------------------

	spark.read.format("jdbc") 

		Options:

			url    => jdbc connection string  (jdbc:mysql://localhost:3306/mydb? .......)
			driver => "com.mysql.jdbc.Driver"
			dbtable
			user
			password
			
	----------------

os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()


   Use-Case
   --------
     Datasets: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens

     From movies.csv and ratings.csv datasets, fetch the top 10 movies with heightest average user rating
	-> Consider only those movies that are rated by atleast 30 users. 
	-> Data required:  movieId, title, totalRatings, averageRating
	-> Sort the data in the DESC order of averageRating
	-> Save the output as a single pipe separated CSV file with header. 
	
        => Please try to solve it.


Import statements
------------------
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import expr, col, column, broadcast
from pyspark.sql.functions import lit, asc, desc, count, sum, min, max, avg, dense_rank, rank, row_number, to_date
from pyspark.sql.functions import round, dense_rank, rank, row_number, to_date
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType
from pyspark.sql.window import Window
----------------------------------------------
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window


   Working with Hive
   -----------------

     => Hive is data warehousing platform on Hadoop

	-> warehouse: Is a directory where hive stores all its data files.
	-> metastore: Is a external service (MySQL) where hive stores all its meta data. 

	spark = SparkSession \
    		.builder \
    		.appName("Datasorces") \
    		.config("spark.master", "local") \
    		.config("spark.sql.warehouse.dir", warehouse_location) \
    		.enableHiveSupport() \
    		.getOrCreate()

----------------------------------------------------

 # -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

spark.catalog.listTables()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinStr = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinStr) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write.mode("overwrite").format("hive").saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()

  ------------------------------------------------
    Spark MLlib & Machine Learning
  ------------------------------------------------

   ML Model => Is a lerned entity
	       Learns from some historic data and is trained by algorithm. 
     	       Algorithms train a model
   
   Terminology
   -----------
    1. Features	     : inputs, dimensions ( what you learn from )
    2. Label	     : output ( what you learn about )
    3. Training Data : data containtaing features and (optionally) label
    4. Algorithm     : Is a mathematical ietartive computation to establish a relation between label and 
		       features with a goal to minimize loss. 
    5. Model	     : Output of the algorithm with an built relation between label and features
    6. Error	     : variation between prediction and actual output
    7. Loss          : Combined error of the entire dataset.    


	X	Y	Z (label) pred. error
	---------------------------------------
	1200	600	3100	3000	100     
	1000	1000	2950	3000	-50
	1100	300	2510	2500	 10
	2000	100	4200	4100	100
	1600	300	3450	3500	-50
	1300	100	 ??
	------------------------------------
			  Loss:  310/5 = 62   (RMSE -> Root mean square error)
 
    Algorithm:
        Model 1 :   z = 2x + y     	   Loss: 62
	Model 2 :   z = 2.1x + 0.9y        Loss: 58
	Model 3 :   z = 2.15x - 0.8y + 10  Loss: 50
	....
	....
	
    
   Types of Machine Learning
   -------------------------
	
	1. Supervised Learning
		Trainined data is labelled data (features & label)

		1.1  Classification
			-> Label is one of two/few fixed values.
			-> Ex: 1/0, [1,2,3,4,5] 
			-> Ex: Email spam, survival prediction

		1.2  Regression
			-> Label is a continuos value
			-> Ex: House Price prediction

	2. Unsupervised Learning
		Training data will have only features & no label

		2.1  Clustering  (Collaborative filtering)

		2.2  Dimesionality reduction	
		
	3. Reinforcement learning		
		-> Semi-supervised learning


   Steps in an ML project
   ----------------------

 	1. Data Collection
		output: raw data

	2. Data Preparation
		output: Prepared data that can be given to an algorithm for training. 
			-> "Feature Vector"

		-> All the data must be numeric.
		-> There should be no nulls/empty string etc. 
		-> Remove all outliers.

	3. Train the model using one or more algorithms
		
     		output: unevaluated model

	4. Evaluate the model
		
		output: evaluated model

		1. Split the prepared data into train (70%) and validation (30%) datasets in the 70:30 ratios
		2. Train the model using train dataset.
		3. Using the model, get predictions on 30% dataset. 
		4. By comparing the label and predictions, we can evaluate the model.

	5. Deploy the model


    Spark MLlib
    -----------
	We have two libraries:		
		pyspark.mllib	-> legacy library (based on RDDs)
		pyspark.ml	-> current library (based on DFs)

	What do you have?
	-----------------		
	1. Feature Tools 	 : Extracting, Transforming, Selecting features
	2. ML Algorithms 	 : Classification, Regression, Clustering, Collaborative Filtering algos.
	3. Pipeline	 	 : Technique of performing ML training in MLlib
	4. Model Selection Tools : CrossValidation, TrainValidation Split (hyperparmeter tuning)
	5. Evaluation Tools	
	6. Utility packages	 : linalg, stats


	Building blocks of Spark MLlib
        ------------------------------
	
	1. DataFrame	: All data is represented by DFs

	2. Feature vector  : Is a vector object containing all the features in numeric format.		
		-> Dense Vector :   Vectors.dense(0,0,0,8,9,0,0,0,0,3,0,0,5,6,0)
		-> Sparse Vector:   Vector.sparse(15, [3,4,9,12,13], [8,9,3,5,6])

        3. Estimator
		-> input: dataframe
		-> output: model
		-> method: fit
			<model> = <estimator>.fit( <df> )
			Ex: All ML algorithms, several Feature tools (RFormula, StringIndexer, OneHotEncoder etc.)

	4. Transformer
		-> input: dataframe
		-> output: dataframe
		-> method: transform
			<outputDf> = <transformer>.transform( <inputDf> )
			Ex: All Models,  several Feature tools (tokenizer, hashingTf etc.)

        5. Pipeline
		-> A set of stages containing Transformers and estimators
		-> Defines a workflow. 

		pl = Pipeline(stages = [T1, T2, T3, E4]) 
		plModel = pl.fit( df )

		df -> T1 -> df2 -> T2 -> df3 -> T3 -> df4 -> E4 -> plModel


   Mini Project
   ------------
	URL: https://www.kaggle.com/c/titanic
	Titanic - Machine Learning from Disaster

PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked
1,0,3,Harris",male,22,1,0,A/5 21171,7.25,,S
2,1,1,"Cumings, Mrs.Braund, Mr. Owen  John Bradley (Florence Briggs Thayer)",female,38,1,0,PC 17599,71.2833,C85,C
3,1,3,"Heikkinen, Miss. Laina",female,26,0,0,STON/O2. 3101282,7.925,,S
4,1,1,"Futrelle, Mrs. Jacques Heath (Lily May Peel)",female,35,1,0,113803,53.1,C123,S
5,0,3,"Allen, Mr. William Henry",male,35,0,0,373450,8.05,,S
6,0,3,"Moran, Mr. James",male,,0,0,330877,8.4583,,Q
7,0,1,"McCarthy, Mr. Timothy J",male,54,0,0,17463,51.8625,E46,S
8,0,3,"Palsson, Master. Gosta Leonard",male,2,3,1,349909,21.075,,S

   	Label: Survived
	Features: Pclass,Sex,Age,SibSp,Parch,Fare,Embarked 
		=> numerical   :  Pclass,Age,SibSp,Parch,Fare
		   categorical :  Sex,Embarked 
			-> StringIndexer, OnHotEncoder

 |-- Survived: float (nullable = true)
 |-- Sex: string (nullable = true)
 |-- Embarked: string (nullable = true)
 |-- Pclass: float (nullable = true)
 |-- Age: float (nullable = true)
 |-- SibSp: float (nullable = true)
 |-- Parch: float (nullable = true)
 |-- Fare: float (nullable = true) 
   indexedSex      
   indexedEmbarked
   sexVec
   embarkedVec
   features    			==> model

  ========================================================
    Introduction Spark Streaming
  ========================================================

     Spark Streaming

	  => microbatch based processing
	  => Provides "seconds" scale latency.  (near-real-time processing)


      StreamingContext :
	  -> Is the starting point of execution
	  -> Defines a micro-batch window
	  -> Each micro-batch is an RDD

       DStream (discretized stream)
	-> Is a continuous flow of RDDs.

	  
  	# Create a local StreamingContext with two threads and batch interval of 1 sec.
	sc = SparkContext("local[2]", "NetworkWordCount")
	ssc = StreamingContext(sc, 1)

	lines = ssc.socketTextStream("localhost", 9999)
	words = lines.flatMap(lambda line: line.split(" "))
	pairs = words.map(lambda word: (word, 1))
	wordCounts = pairs.reduceByKey(lambda x, y: x + y)
	wordCounts.print()

	ssc.start()             
	ssc.awaitTermination() 




















